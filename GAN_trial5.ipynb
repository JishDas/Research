{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow_docs.vis import embed\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_channels = 1\n",
    "num_classes = 20\n",
    "image_size = 20\n",
    "latent_dim = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We'll use all the available examples from both the training and test\n",
    "# # sets.\n",
    "# (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "# all_digits = np.concatenate([x_train, x_test])\n",
    "# all_labels = np.concatenate([y_train, y_test])\n",
    "\n",
    "# # Scale the pixel values to [0, 1] range, add a channel dimension to\n",
    "# # the images, and one-hot encode the labels.\n",
    "# all_digits = all_digits.astype(\"float32\") / 255.0\n",
    "# all_digits = np.reshape(all_digits, (-1, 28, 28, 1))\n",
    "# all_labels = keras.utils.to_categorical(all_labels, 10)\n",
    "\n",
    "# # Create tf.data.Dataset.\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((all_digits, all_labels))\n",
    "# dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# print(f\"Shape of training images: {all_digits.shape}\")\n",
    "# print(f\"Shape of training labels: {all_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset generator - generating symmetric matrices\n",
    "dimension_2D=20\n",
    "n_samples=20\n",
    "start=0\n",
    "end=20\n",
    "\n",
    "# Extracting the distance matrix\n",
    "f1=open('Dataset1.txt','r')\n",
    "l1=[]\n",
    "l1=[line.split() for line in f1]\n",
    "distance=[[] for i in range(len(l1)//20)]\n",
    "i1=0\n",
    "j1=0\n",
    "while True:\n",
    "    distance[i1].append(list(float(x) for x in l1[j1]))\n",
    "    if((j1+1)%20==0):\n",
    "        i1+=1\n",
    "    if(j1==len(l1)-1):\n",
    "        break\n",
    "    j1+=1\n",
    "\n",
    "# Extracting the adjency matrix without distance\n",
    "f2=open('Dataset2.txt','r')\n",
    "l2=[]\n",
    "l2=[line.split() for line in f2]\n",
    "network=[[] for i in range(len(l2)//20)]\n",
    "i2=0\n",
    "j2=0\n",
    "while True:\n",
    "    network[i2].append(list(float(x) for x in l2[j2]))\n",
    "    if((j2+1)%20==0):\n",
    "        i2+=1\n",
    "    if(j2==len(l2)-1):\n",
    "        break\n",
    "    j2+=1\n",
    "\n",
    "distance=np.array(distance)\n",
    "network=np.array(network)\n",
    "network = network.astype('float32')\n",
    "network = np.expand_dims(network, axis=-1)\n",
    "distance = np.expand_dims(distance,axis= -1)\n",
    "distance = distance.astype('float32')\n",
    "\n",
    "# Extracting the number of nodes and adding as labels\n",
    "# Data distance is a 1000*2 matrix which contains 1000 20*20*1 matrices\n",
    "f3=open('Dataset_Labels.txt','r')\n",
    "labels=[l.split() for l in f3]\n",
    "\n",
    "# Adding labels to the distance matrix\n",
    "data_distance = []\n",
    "data_distance_labels = []\n",
    "for i in range(1000):\n",
    "    data_distance.append((distance[i]))\n",
    "    data_distance_labels.append((int(labels[i][0])))\n",
    "data_distance = np.array(data_distance,dtype=object)\n",
    "\n",
    "# Adding labels to network matrix\n",
    "# Data network is a 1000*2 matrix which contains 1000 20*20*1 matrices\n",
    "\n",
    "data_network_train = []\n",
    "data_network_labels_train=[]\n",
    "for i in range(800):\n",
    "    data_network_train.append(network[i])\n",
    "    data_network_labels_train.append((int(labels[i][0])))\n",
    "data_network_train = np.array(data_network_train)\n",
    "data_network_train = data_network_train.astype('float32')\n",
    "data_network_labels_train = np.array(data_network_labels_train)\n",
    "data_network_labels_train = data_network_labels_train.astype('float32')\n",
    "\n",
    "data_network_test = []\n",
    "data_network_labels_test=[]\n",
    "for i in range(800,1000):\n",
    "    data_network_test.append(network[i])\n",
    "    data_network_labels_test.append((int(labels[i][0])))\n",
    "data_network_test = np.array(data_network_test)\n",
    "data_network_test = data_network_test.astype('float32')\n",
    "data_network_labels_test = np.array(data_network_labels_test)\n",
    "data_network_labels_test = data_network_labels_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420 21\n"
     ]
    }
   ],
   "source": [
    "generator_in_channels = latent_dim + num_classes\n",
    "discriminator_in_channels = num_channels + num_classes\n",
    "print(generator_in_channels, discriminator_in_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the discriminator.\n",
    "in_shape=(dimension_2D,dimension_2D,1)\n",
    "discriminator = keras.Sequential(\n",
    "    [\n",
    "        layers.Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=in_shape),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n",
    "\n",
    "# Create the generator.\n",
    "generator = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer((generator_in_channels,)),\n",
    "        # We want to generate 128 + num_classes coefficients to reshape into a\n",
    "        # 5x5x(128 + num_classes) map.\n",
    "        layers.Dense(5 * 5 * generator_in_channels),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Reshape((5, 5, generator_in_channels)),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(1, (5, 5), padding=\"same\", activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConditionalGAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(ConditionalGAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.gen_loss_tracker = keras.metrics.Mean(name=\"generator_loss\")\n",
    "        self.disc_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.gen_loss_tracker, self.disc_loss_tracker]\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(ConditionalGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data.\n",
    "        real_images, one_hot_labels = data\n",
    "\n",
    "        # Add dummy dimensions to the labels so that they can be concatenated with\n",
    "        # the images. This is for the discriminator.\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        image_one_hot_labels = tf.repeat(\n",
    "            image_one_hot_labels, repeats=[image_size * image_size]\n",
    "        )\n",
    "        image_one_hot_labels = tf.reshape(\n",
    "            image_one_hot_labels, (-1, image_size, image_size, num_classes)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space and concatenate the labels.\n",
    "        # This is for the generator.\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_vector_labels = tf.concat(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\n",
    "        )\n",
    "\n",
    "        # Decode the noise (guided by labels) to fake images.\n",
    "        generated_images = self.generator(random_vector_labels)\n",
    "\n",
    "        # Combine them with real images. Note that we are concatenating the labels\n",
    "        # with these images here.\n",
    "        fake_image_and_labels = tf.concat([generated_images, image_one_hot_labels], -1)\n",
    "        real_image_and_labels = tf.concat([real_images, image_one_hot_labels], -1)\n",
    "        combined_images = tf.concat(\n",
    "            [fake_image_and_labels, real_image_and_labels], axis=0\n",
    "        )\n",
    "\n",
    "        # Assemble labels discriminating real from fake images.\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "\n",
    "        # Train the discriminator.\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space.\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_vector_labels = tf.concat(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\n",
    "        )\n",
    "\n",
    "        # Assemble labels that say \"all real images\".\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_images = self.generator(random_vector_labels)\n",
    "            fake_image_and_labels = tf.concat([fake_images, image_one_hot_labels], -1)\n",
    "            predictions = self.discriminator(fake_image_and_labels)\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        # Monitor loss.\n",
    "        self.gen_loss_tracker.update_state(g_loss)\n",
    "        self.disc_loss_tracker.update_state(d_loss)\n",
    "        return {\n",
    "            \"g_loss\": self.gen_loss_tracker.result(),\n",
    "            \"d_loss\": self.disc_loss_tracker.result(),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_network_labels_train = keras.utils.to_categorical(data_network_labels_train, num_classes)\n",
    "data_network_labels_test = keras.utils.to_categorical(data_network_labels_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-05 21:15:18.753348: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 537600000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/jishnu/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/jishnu/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/jishnu/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/tmp/ipykernel_13370/1953686470.py\", line 38, in train_step\n        random_vector_labels = tf.concat(\n\n    ValueError: Shape must be rank 2 but is rank 5 for '{{node concat}} = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32](random_normal, IteratorGetNext:1, concat/axis)' with input shapes: [?,400], [?,21,20,20,20], [].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/jishnu/Desktop/DsAlgo/Research/GAN_trial5.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jishnu/Desktop/DsAlgo/Research/GAN_trial5.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m cond_gan\u001b[39m.\u001b[39mcompile(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jishnu/Desktop/DsAlgo/Research/GAN_trial5.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     d_optimizer\u001b[39m=\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39m\u001b[39m0.0003\u001b[39m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jishnu/Desktop/DsAlgo/Research/GAN_trial5.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     g_optimizer\u001b[39m=\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39m\u001b[39m0.0003\u001b[39m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jishnu/Desktop/DsAlgo/Research/GAN_trial5.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     loss_fn\u001b[39m=\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mBinaryCrossentropy(from_logits\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jishnu/Desktop/DsAlgo/Research/GAN_trial5.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jishnu/Desktop/DsAlgo/Research/GAN_trial5.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# fit the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jishnu/Desktop/DsAlgo/Research/GAN_trial5.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m model \u001b[39m=\u001b[39m cond_gan\u001b[39m.\u001b[39;49mfit(data_network_train, data_network_labels_train,batch_size\u001b[39m=\u001b[39;49mbatch_size,epochs\u001b[39m=\u001b[39;49m \u001b[39m20\u001b[39;49m,verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,validation_data\u001b[39m=\u001b[39;49m(data_network_test, data_network_labels_test))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileq_l3ybrf.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;32m/home/jishnu/Desktop/DsAlgo/Research/GAN_trial5.ipynb Cell 9\u001b[0m in \u001b[0;36mConditionalGAN.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jishnu/Desktop/DsAlgo/Research/GAN_trial5.ipynb#W6sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m batch_size \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mshape(real_images)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jishnu/Desktop/DsAlgo/Research/GAN_trial5.ipynb#W6sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m random_latent_vectors \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mnormal(shape\u001b[39m=\u001b[39m(batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlatent_dim))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jishnu/Desktop/DsAlgo/Research/GAN_trial5.ipynb#W6sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m random_vector_labels \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mconcat(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jishnu/Desktop/DsAlgo/Research/GAN_trial5.ipynb#W6sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     [random_latent_vectors, one_hot_labels], axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jishnu/Desktop/DsAlgo/Research/GAN_trial5.ipynb#W6sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jishnu/Desktop/DsAlgo/Research/GAN_trial5.ipynb#W6sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# Decode the noise (guided by labels) to fake images.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jishnu/Desktop/DsAlgo/Research/GAN_trial5.ipynb#W6sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m generated_images \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerator(random_vector_labels)\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/jishnu/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/jishnu/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/jishnu/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/tmp/ipykernel_13370/1953686470.py\", line 38, in train_step\n        random_vector_labels = tf.concat(\n\n    ValueError: Shape must be rank 2 but is rank 5 for '{{node concat}} = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32](random_normal, IteratorGetNext:1, concat/axis)' with input shapes: [?,400], [?,21,20,20,20], [].\n"
     ]
    }
   ],
   "source": [
    "cond_gan = ConditionalGAN(\n",
    "    discriminator=discriminator, generator=generator, latent_dim=latent_dim\n",
    ")\n",
    "cond_gan.compile(\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    ")\n",
    "\n",
    "# fit the model\n",
    "model = cond_gan.fit(data_network_train, data_network_labels_train,batch_size=batch_size,epochs= 20,verbose=1,validation_data=(data_network_test, data_network_labels_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ea7a77ecd2a485eceffd449bc1af6fd531f9977eeaf62715f4447faafab5e7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
